{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<img src=\"Images/IMG-Wine-Alcohol_Banner-English.png\" alt=\"Title Banner\" style=\"display: block; margin-left: auto; margin-right: auto; width: 100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "The class of wine can be found in the notebook \"Classification of Wine\". But sometimes other properties of the wine are of interest which cannot be divided into a finite number of classes. In this notebook we therefore deal with the prediction of the alcohol content of wine using regression analysis.\n",
    "\n",
    "<img src=\"Images/IMG-sklearn-logo.png\" alt=\"Title Banner\" style=\"float:right; display: block; margin-left: auto; margin-right: auto; width: 30%\">\n",
    "\n",
    "For this purpose we will use some regression models and other functions from the Python package [```scikit-learn```](https://scikit-learn.org/stable/user_guide.html) (short: ```sklearn```). ``Sklearn`` is an extremely powerful framework that provides many machine learning methods with uniform interfaces so that you can try out different models quickly and easily. In the course of this notebook, you will become familiar with some of the functions and objects used in regression. In the notebook _\"Classification of Wine\"_ you will also work with ```sklearn```, but with a focus on the methods and objects for solving classification problems.\n",
    "\n",
    "It is very recommended for both notebooks to always get further information about the objects used in the documentation and the User's Guide of ``sklearn`` in order to acquire a deeper understanding of the context.\n",
    "\n",
    "## Content\n",
    "<table style=\"width:256; border: 1px solid black; display: inline-block\">\n",
    "  <tr>\n",
    "    <td  style=\"text-align:right\" width=64px><img src=\"Images/IMG-csv-in.png\" style=\"float:left\"></td>\n",
    "      <td style=\"text-align:left\" width=128px>\n",
    "          <a style=\"color:black; font-size:14px; font-weight:bold; text-decoration:none\" href='#import_data'>Importing data</a>\n",
    "      </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"text-align:right\"><img src=\"Images/IMG-magnifying-glass.png\" style=\"float:left\"></td>\n",
    "    <td style=\"text-align:left\" width=128px><a style=\"color:black; font-size:14px; font-weight:bold; text-decoration:none\" href='#analyze_data'>Analysing data</a>\n",
    "      </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td style=\"text-align:right\"><img src=\"Images/IMG-broom.png\" style=\"float:left\"></td>\n",
    "    <td style=\"text-align:left\" width=128px><a style=\"color:black; font-size:14px; font-weight:bold; text-decoration:none\" href='#clean_data'>Preprocessing data</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td style=\"text-align:right\"><img src=\"Images/IMG-gears.png\" style=\"float:left\"></td>\n",
    "    <td style=\"text-align:left\" width=128px><a style=\"color:black; font-size:14px; font-weight:bold; text-decoration:none\" href='#build_model'>Choosing a model</a>\n",
    "        </td>\n",
    "        <tr>\n",
    "    <td style=\"text-align:right\"><img src=\"Images/IMG-new-file-out.png\" style=\"float:left\"></td>\n",
    "    <td style=\"text-align:left\" width=128px><a style=\"color:black; font-size:14px; font-weight:bold; text-decoration:none\" href='#save_model'>Saving the model</a>\n",
    "        </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "**Note:** In this notebook random numbers are used at some point (e.g. the division into training and test sets). This can mean that some results are not exactly reproducible for you and that the relative ranking of the various regression models or the optimal values of some hyperparameters may look slightly different. However, this does not change anything in the general procedure or the programming patterns.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a id='import_data'></a><div><img src=\"Images/IMG-csv-in.png\" style=\"float:left\"> <h2 style=\"position: relative; top: 6px; left:5px\">1. Importing data</h2>\n",
    "<p style=\"position: relative; top: 10px\">\n",
    "The data of the wines were collected as part of a <a href='#data_source'> scientific study </a> and the exact meaning of the individual characteristics can be found in the corresponding paper. The listed features are:\n",
    "\n",
    "<table style=\"width:256; border: 1px solid black; display: inline-block\">\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Fixed acidity</p>\n",
    "    <tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Volatile acidity</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Citric acid</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Residual sugar</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Chlorides</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Free sulfur dioxide</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Total sulfur dioxide</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Density</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">pH</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Sulphates</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Alcohol</p>\n",
    "    </tr>\n",
    "        <td style=\"text-align:left\"><p style=\"color:black; font-size:14px; font-weight:bold\">Quality</p>\n",
    "    </table>\n",
    "\n",
    "<a id='data_source'></a><b>Source of the data:</b> P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis.\n",
    "  Modeling wine preferences by data mining from physicochemical properties.\n",
    "  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We have two sets of data, one of which contains only red wines and the other only white wines. For this notebook, let's assume that the color of the wine has no influence on the result.\n",
    "\n",
    "The first step is therefore to import and connect the two data sets. The data are located in ``Data/winequality-red.csv`` and ```Data/winequality-white.csv```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-b4f978fa82e7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-2-b4f978fa82e7>\"\u001B[1;36m, line \u001B[1;32m4\u001B[0m\n\u001B[1;33m    df_red =\u001B[0m\n\u001B[1;37m            ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # See Preprocessing/Lego-Sets/Lego Sets Preprocessing.ipynb for an introduction to Pandas\n",
    "\n",
    "# Import datasets (pandas function: pd.read_csv() Attention: Semicolon is used as separator ';'!)\n",
    "df_red =\n",
    "df_white =\n",
    "# Connect the datasets with pd.concat()\n",
    "df ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a id='analyze_data'></a><div><img src=\"Images/IMG-magnifying-glass.png\" style=\"float:left\"> <h2 style=\"position: relative; top: 6px; left:5px\" >2. Analysing data</h2>\n",
    "    \n",
    "<p style=\"position: relative; top: 10px\">\n",
    "Next, let's take a closer look at the data. Do we need a feature conversion? Do values have to be filled? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a bit of descriptive statistics (pandas function: describe()):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We note three important facts:\n",
    "\n",
    "1. There are a total of 6497 wines for which we have data (__```count```__), at least there are as many entries in each column. Thus, we don't have to fill in any data fortunately!\n",
    "2. All features are purely numerical. So we don't have to convert any data either!\n",
    "3. The data are scaled very differently because the mean values (__```mean```__) and standard deviations (__```std```__) spread over several orders of magnitude.\n",
    "\n",
    "It is therefore advisable to first standardize the columns individually so that they each have a mean value of 0 and a standard deviation of 1 (also called $z$-score normalization). This brings all the features into the same order of magnitude without simplifying the relative distribution.\n",
    "\n",
    "So our roadmap for cleaning the data is clear:\n",
    "- No converting or filling necessary\n",
    "- Standardization of the data is recommended\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<a id='clean_data'></a><div><img src=\"Images/IMG-broom.png\" style=\"float:left\"> <h2 style=\"position: relative; top: 6px; left:5px\">3. Preprocessing data</h2>\n",
    "<p style=\"position: relative; top: 10px\">\n",
    "As stated above, we don't need to do any conversions or replacements, just standardize. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "However, before we carry out the standardization, we should first split the data into training and test sets so that we do not already use data during standardization that should actually be test data \"unseen\" by the model. To do this, we first divide the data set into the feature vector sequence $X$ and class label $y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we separate the entire data set into features and the characteristic to be predicted\n",
    "X = df.drop(columns=['alcohol']) # All columns of df except 'alcohol'\n",
    "y = df['alcohol'] # Only column 'alcohol'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now comes the actual split in the train and test set, which we can do with the function [```train_test_split()```](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function train_test_split from the sklearn module model_selection\n",
    "from PAKET.MODUL import FUNCTION\n",
    "\n",
    "# Now use the train_test_split() function to get a holdout test set of 20% of the total data\n",
    "X_train, X_test, y_train, y_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now we can determine the parameters for the standardization on the training set and execute them on training and test data. We could of course manually subtract the mean in each column and divide it by its standard deviation. But it is more elegant with the help of the object [```StandardScaler```](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the StandardScaler object from the sklearn preprocessing module\n",
    "from PAKET.MODUL import OBJEKT\n",
    "\n",
    "# Instantiate a StandardScaler() object\n",
    "stdScaler =\n",
    "\n",
    "# Calculate the standardization parameters on the training set (!) X_train using the \"fit()\" function\n",
    "\n",
    "# Apply the **same** standardization to the training and test data using the transform() method\n",
    "X_train =\n",
    "X_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "<a id='build_model'></a><div><img src=\"Images/IMG-gears.png\" style=\"float:left\"> <h2 style=\"position: relative; top: 6px; left:5px\">4. Modellauswahl</h2>\n",
    "<p style=\"position: relative; top: 10px\">\n",
    "Now we can try out the regression approaches known from the lecture one after the other to find the optimal model. </p>\n",
    "\n",
    "The simplest approach to regression of the alcohol content known from the lecture is linear regression. For this purpose there is the object [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) in ```sklearn```, which allows a very simple implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the LinearRegression object from the linear_model module in the sklearn package\n",
    "from PAKET.MODUL import OBJEKT\n",
    "\n",
    "# Instantiate a model with the standard parameters\n",
    "lr1_model = LinearRegression ()\n",
    "# Train (\"Fit\") the model on the training data X_train using the fit () method\n",
    "lr1_model.fit (X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "You can check the quality of this regression model (and also every other model in ```sklearn```) on any data set with the method [```score()```](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score). For regression models, the standard metric used here is the coefficient of determination $R^2$ (see lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 in training\n",
    "train_score_lr1 = lr1_model.score(X_train, y_train)\n",
    "# R^2 in the test\n",
    "test_score_lr1 = lr1_model.score(X_test, y_test)\n",
    "# Output of the performance measures\n",
    "print(\"1st order linear regression - training: R² = {:.2f} Test: R² = {:.2f}\".format(train_score_lr1, test_score_lr1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the lecture you learned that one can assume different basis functions $\\varphi(x)$ in linear regression. You can also use the ```LinearRegression``` object with any of the functions $\\varphi(x)$. You have to carry out the transformation of the characteristics mediated by $\\varphi(x)$ yourself. Hand over the new, transformed characteristics (i.e. the matrix $\\Phi$ in the lecture) to the fit method of ```LinearRegression``` afterwards.\n",
    "\n",
    "We will try that out here using the example of a polynomial basis function of the second degree:\n",
    "\n",
    "\\begin{align}\n",
    "\\varphi_1(x) &= x^1 \\\\\n",
    "\\varphi_2(x) &= x^2\n",
    "\\end{align}\n",
    "\n",
    "The characteristics of first order ($x^1$) are already in the training data. We now have to add the second order features ($x^2$) manually by squaring the columns of the feature matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For the matrix operations\n",
    "\n",
    "# The first-order feature matrix must be supplemented by the second-order features\n",
    "X_2_train = np.concatenate([X_train, X_train ** 2], axis=1)\n",
    "\n",
    "# Do the same for the test data\n",
    "X_2_test = np.concatenate([X_test, X_test ** 2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The 2nd order features are not necessarily mean-free and have a variance different from 1. So we still have to standardize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new standard scaler and \"fit\" it into the training data including the 2nd order\n",
    "stdScaler_2 = StandardScaler().fit(X_2_train)\n",
    "\n",
    "# Standardize the second-order training and test data\n",
    "X_2_train = stdScaler_2.transform(X_2_train)\n",
    "X_2_test = stdScaler_2.transform(X_2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "So now we can train a regression model for the features up to order 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model with the standard parameters\n",
    "lr2_model =\n",
    "# Train (\"fit\") the model on the training data X_2_train using the fit() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 in training\n",
    "train_score_lr2 =\n",
    "# R^2 in the test\n",
    "test_score_lr2 =\n",
    "# Output of the performance measures\n",
    "print(\"2nd order linear regression - training: R² = {:.2f} Test: R² = {:.2f}\".format( XXX, YYY ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In fact, the prediction has improved significantly! However, we have now of course also used more predictors, which fundamentally increases the risk of overfitting. So as a control we calculate the corrected coefficient of determination $ R^2_\\mathrm{korr} = 1 - (1 - R^2)\\frac{N-1}{N-M-1} $ (see lecture):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N = X_2_train.shape [0]\n",
    "# Number of predictors\n",
    "M = X_2_train.shape [1]\n",
    "# Calculate corrected R^2\n",
    "R2_korr = 1 - (1-train_score_lr2) * (N-1) / (N-M-1)\n",
    "print (\"The corrected R² for the training data is {:.2f}.\".format(R2_korr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Since $R^2 = R^2_\\mathrm{korr}$, the increase in the number of predictors is apparently still justified due to the large number of observations.\n",
    "\n",
    "Would adding 3rd order features provide even more improvement? Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The feature matrices of the 2nd order must be supplemented by the features of the 3rd order\n",
    "X_3_train =\n",
    "X_3_test =\n",
    "\n",
    "# Instantiate a new standard scaler and \"fit\" it into the training data including 3rd order\n",
    "stdScaler_3 =\n",
    "\n",
    "# Standardize the third-order training and test data\n",
    "X_3_train =\n",
    "X_3_test =\n",
    "\n",
    "# Instantiate a model with the standard parameters\n",
    "lr3_model =\n",
    "# Train (\"Fit\") the model on the training data X_3_train using the fit() method\n",
    "\n",
    "# R^2 in training\n",
    "train_score_lr3 =\n",
    "# R^2 in the test\n",
    "test_score_lr3 =\n",
    "# Output of the performance measures\n",
    "print (\"3rd order linear regression - training: R² = {:.2f} Test: R² = {:.2f}\".format ( XXX, YYY ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Note:** With regression models of higher order, one often uses not only the features in the respective power, but also the so-called \"interaction terms\", i.e. the pairwise multiplications of the features. For only two features $(x_1, x_2)$ this would result in the following model:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 \\cdot x_1 + w_2 \\cdot x_1 \\cdot x_2 + w_3 \\cdot x_1^2 + w_4\\cdot x_2 + w_5\\cdot x_2^2$$\n",
    "\n",
    "With a larger number of features, the feature transformation becomes very complex. That is why ```sklearn``` offers the object [```PolynomialFeatures```](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) with which this transformation can be implemented much more easily. Take a look at the linked documentation and try to do the transformation for the second and third order with it!\n",
    "\n",
    "But first we continue with the $L^2$ regularized regression, also called ridge regression. For that, ```sklearn``` offers an object [```Ridge```](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html). However, you must explicitly pass the value for ```alpha``` (in the lecture $\\lambda$) to this object. This value usually first has to be found through cross-validation. For this reason there is also the object [```RidgeCV```](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html), with which you can cross-validate to determine ```alpha``` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the RidgeCV object from the linear_model module in the sklearn package\n",
    "from PAKET.MODUL import OBJEKT\n",
    "\n",
    "# Select the values for the regularization parameter alpha (called lambda in the lecture) that you want to try out\n",
    "alpha_candidates = np.logspace(-6, 10, 17) # usually a logarithmic range of values is used here as candidates\n",
    "\n",
    "# Instantiate a RidgeCV object passing in the candidates for alpha\n",
    "ridge1_model = RidgeCV(alphas=alpha_candidates)\n",
    "\n",
    "# Train (\"fit\") the model on the training data\n",
    "ridge1_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see, compared to LinearRegression, the programming pattern repeats itself a lot, even though you are using a different model. After training, RidgeCV will give you back the optimally regularized model. The optimal value found for ```alpha``` can be read out via the ```alpha_``` parameter of the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal alpha\n",
    "alpha_opt = ridge1_model.alpha_\n",
    "# R^2 in training\n",
    "train_score_ridge1 = ridge1_model.score(X_train, y_train)\n",
    "# R^2 in the test\n",
    "test_score_ridge1 = ridge1_model.score(X_test, y_test)\n",
    "# Output of the performance measures\n",
    "print(\"Ridge regression 1st order (alpha={:.2f}): Training R ^ 2 = {:.2f} Test R^2 = {: .2f}\".format(alpha_opt, train_score_ridge1, test_score_ridge1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Proceed in the same way for the characteristics up to order 2 and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a RidgeCV object passing in the candidates for alpha\n",
    "ridge2_model =\n",
    "\n",
    "# Train (\"fit\") the model on the training data\n",
    "\n",
    "\n",
    "# Optimal alpha\n",
    "alpha_opt =\n",
    "# R^2  in training\n",
    "train_score_ridge2 =\n",
    "# R^2 in the test\n",
    "test_score_ridge2 =\n",
    "# Output of the performance measures\n",
    "print(\"2nd order ridge regression(alpha = {:.2f}): Training R^2 = {:.2f} Test R ^ 2 = {:.2f}\".format(XXX, YYY, ZZZ))\n",
    "# Instanziieren Sie ein RidgeCV-Objekt und übergeben Sie dabei die Kandidaten für alpha\n",
    "ridge2_model = \n",
    "\n",
    "\n",
    "# Instantiate a RidgeCV object passing in the candidates for alpha\n",
    "ridge3_model =\n",
    "\n",
    "# Train (\"fit\") the model on the training data\n",
    "\n",
    "\n",
    "# Optimal alpha\n",
    "alpha_opt =\n",
    "# R^2 in training\n",
    "train_score_ridge3 =\n",
    "# R^2 in the test\n",
    "test_score_ridge3 =\n",
    "# Output of the performance measures\n",
    "print(\"3rd order ridge regression (alpha = {:.2f}): Training R^2 = {: .2f} Test R^2 = {:.2f}\".format(XXX, YYY, ZZZ))\n",
    "# Instanziieren Sie ein RidgeCV-Objekt und übergeben Sie dabei die Kandidaten für alpha\n",
    "ridge3_model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We find that the optimal $\\lambda$ for 1st and 2nd order is equal to 1, which is mathematically identical to the irregularized regression (see lecture). A very strong regularization is identified as optimal for the 3rd order. However, the performance on the test set is still worse than with the 2nd order. Obviously, with this set of observations, overfitting cannot be avoided even with $L2$ regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the lecture it was mentioned that Support Vector Machines (SVM) can also be used for regression. Even if these methods are not the subject of the lecture and their theoretical background would go beyond the scope at this point, we want to try them out because they represent a very powerful model and can be implemented without deeper knowledge through ```sklearn```.\n",
    "\n",
    "The (very simplified) basic idea of regression with support vector machines is that, similar to the classification, the function to be found is not based on all data points, but only on a few support vectors through which the regression function is then placed. A parameter $\\epsilon$ controls how many points are still taken into account, whereby a larger $\\epsilon$ means a lower sensitivity to outliers. You can also use the kernel trick (see lecture) with regression and regularize it with the parameter $C$.\n",
    "\n",
    "The object for regression with support vector machines in ```sklearn``` is called [```SVR```](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html?highlight=svr#sklearn.svm.SVR):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SVR object from the svm module in the sklearn package\n",
    "\n",
    "\n",
    "# Instantiating an SVR object with default settings\n",
    "svr_model =\n",
    "# Train the model\n",
    "\n",
    "\n",
    "# R^2 in training\n",
    "train_score_svr =\n",
    "# R^2 in the test\n",
    "test_score_svr =\n",
    "\n",
    "# Output of the performance\n",
    "print(\"Support Vector Regression: Training R^2 = {:.2f} Test R^2 = {:.2f}\".format(XXX, YYY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "As you can see, the SVR achieved a very good test result \"right from the start\". However, it is usually worthwhile to optimize the hyperparameters $C$ and $\\epsilon$ with the help of cross-validation.\n",
    "\n",
    "To optimize the hyperparameters (\"tuning the model\") based on cross-validation, ```sklearn``` offers us the object [```GridSearchCV```](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) (in addition to a few other variants) in the module ```model_selection```. This object carries out a grid search on a specified parameter grid and checks the performance of the model parameterized in this way at each grid node with the aid of cross validation (CV). This requires the regression model to be tuned, the parameter grid and other optional control parameters, such as the number of folds in the cross-validation.\n",
    "\n",
    "The values of the hyperparameters to be tried out are transferred in the form of a list from [Dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries). Each dictionary contains a grid that is searched.\n",
    "\n",
    "At the end, the model is trained on all folds with the set of hyperparameters identified as optimal, so that the return value of the ```fit()``` method represents a regression model that is also optimal in terms of the hyperparameters.\n",
    "\n",
    "**It is highly recommended to carry out the hyperparameter optimization on a parallel computer!** Since every hyperparameter configuration and every cross validation can be calculated independently of each other, an almost perfect parallel [Speedup](https://de.wikipedia.org/wiki/Speedup) is possible. A simple example (applies analogously to hyperparameters in regression):\n",
    "\n",
    "You want to check which number between 1 and 10 represents the best number of neighbors in the $k$ Nearest Neighbors classifier. To do this, you have to calculate 10 models with 10 different values for $k$ and compare them with one another. For comparison, you want to validate each model with 4 folds. To do this, you have to calculate 4 models for each of the 10 values of $k$, for a total of 40 training processes. On a single CPU computer, this then takes one minute, for example. Since all calculations can be made separately, the exact same training can be performed on a parallel computer with e.g. 20 CPUs. This will take 1/20 of the time, in this example only 3 seconds. If further hyperparameters are to be optimized or more values are to be tried, the computing effort explodes accordingly due to the combinatorial diversity of the grid. So parallel execution is extremely important here. ***We therefore strongly recommend working on the HPC (see [Einführung ins Praktikum](https://bildungsportal.sachsen.de/opal/auth/RepositoryEntry/23165501449/CourseNode/101490883666782))!***\n",
    "\n",
    "**Attention:** The duration of the training depends heavily on the size of the parameter $ C $. For small values of $C$, the training can be completed after a few seconds, while for large values it can take several minutes. So don't be surprised if the cross-validation appears \"frozen\" in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the GridSearchCV object from the model_selection module in the sklearn package\n",
    "from PAKET.MODUL import OBJEKT\n",
    "\n",
    "# Selection of the hyperparameters to be tested (all unspecified ones remain at the default value)\n",
    "param_grid = [\n",
    "     {'C': np.logspace (-2, 2, 5),          # parameter C (see lecture)\n",
    "      'epsilon': np.logspace (-3, 3, 7),    # parameter epsilon for control\n",
    "      'kernel': ['linear', 'rbf']},         # Kernel used\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If you are not working on the HPC and do not want to wait that long, skip the next cell and simply continue working with the following values for the hyperparameters:\n",
    "\n",
    "$C = 10$, $\\epsilon = 0.1$, Kernel: RBF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the \"GridSearchCV\" object\n",
    "svr_model = GridSearchCV(SVR(),           # regression model to be used\n",
    "                          param_grid,     # Hyperparameter values to be tried out\n",
    "                          scoring='r2',   # Metric to use\n",
    "                          cv=5,           # number of folds for cross validation\n",
    "                          verbose=10,     # amount of output during the search (larger number -> more info)\n",
    "                          n_jobs=-1)      # Number of CPUs used in parallel; -1: use all available CPUs\n",
    "\n",
    "# Train the model\n",
    "svr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the first line of the output of the fit function, note the large number of models to be trained (\"fits\") that resulted from the relatively few hyperparameters.\n",
    "\n",
    "You can read out the best hyper parameter combination found using the property ```best_params_``` of the instance of ```GridSearchCV```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svr_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can see whether the performance has improved by comparing the $R^2$ on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr_model = SVR(C=10, epsilon=0.1, kernel='rbf').fit(X_train, y_train) # Uncomment this line if you skipped the cross-validation\n",
    "\n",
    "# R^2 in training\n",
    "train_score_svr =\n",
    "# R^2 in the test\n",
    "test_score_svr =\n",
    "\n",
    "# Output of the performance\n",
    "print (\"Tuned Support Vector Regression: Training R^2 = {:.2f} Test R^2 = {:.2f}\".format (XXX, YYY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The performance has thus improved further through the hyper parameter tuning! This makes the SVR the best of the models considered. Before we export it for further use, it is worth taking a quick look at what is known as the target response plot. The target variable (the actual alcohol content) is plotted against the response (the predicted alcohol content) as an independent variable in a scatter diagram. A perfect regression in this representation would be a straight line with a slope of 1.\n",
    "\n",
    "For the graphical representation, we first import a package for the graphical representation of data. There are a variety of such packages in Python, but we will use [matplotlib](https://matplotlib.org/) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of the module for plotting from Matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# This is a so-called \"magic command\" that enables the plots to be displayed directly in the notebook.\n",
    "# If you forget this, the plots won't show up!\n",
    "% matplotlib inline\n",
    "\n",
    "# You can generate predictions of a model with the predict () method\n",
    "y_predicted = svr_model.predict(X_test)\n",
    "\n",
    "# Plot the sizes against each other in the scatter plot\n",
    "plt.scatter(y_test, y_predicted)\n",
    "plt.xlabel(\"Target Alcohol\")\n",
    "plt.ylabel(\"Predicted Alcohol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The target-response plot does not show a perfect unit line, but the point cloud is relatively evenly distributed and no strong outliers can be seen. The impression thus coincides with the value of $R^2$ in the test and consequently a good result was achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "<a id='save_model'></a><div><img src=\"Images/IMG-new-file-out.png\" style=\"float:left\"> <h2 style=\"position: relative; top: 6px; left:5px\">5. Saving the model</h2>\n",
    "<p style=\"position: relative; top: 10px\">\n",
    "The optimally tuned support vector regression is therefore the best examined model. We would therefore like to save it so that the model does not always have to be retrained for future applications. </p>\n",
    "\n",
    "To do this, we import the ```pickle``` module and write the model and (very important) the object for standardization in a binary file. If we didn't also save the standardizer, a user of our model would not be able to standardize new data and thus make no meaningful predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pickle module\n",
    "import pickle\n",
    "\n",
    "with open('wine_alcohol_model.pickle', 'wb') as model_file:\n",
    "     # Write the two objects to the file\n",
    "     pickle.dump([svr_model, stdScaler], model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "If you want to use the model again elsewhere, you can load it directly from the binary file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wine_alcohol_model.pickle', 'rb') as model_file:\n",
    "    # Extract the objects from the file into two new variables\n",
    "    svr, scaler = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "That's it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<img src=\"Images/IMG-xkcd-ballmer.png\" alt=\"Title Banner\" style=\"display: block; margin-left: auto; margin-right: auto; width: 100%; max-width:652px\">\n",
    "Source: <a href=\"https://xkcd.com/license.html\">xkcd</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "<div>Wine data from <a href=\"http://archive.ics.uci.edu/ml/datasets/Wine\">UCI Machine Learning Repository</a></div>\n",
    "<div>Icons made by <a href=\"https://www.flaticon.com/authors/swifticons\" title=\"Swifticons\">Swifticons</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a></div>\n",
    "<div>Notebook erstellt von Yifei Li und <a href=\"mailto:simon.stone@tu-dresden.de?Subject=Frage%20zu%20Jupyter%20Notebook%20Titanic\" target=\"_top\">Simon Stone</a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}